classdef bp_nn
    properties
        %学习率
        learning_rate =  0.1;
        %输入层尺寸
        input_dim = 2;
        %隐含层尺寸
        h1_dim = 4;
        %输出层尺寸
        output_dim = 3;
    end
    methods
        %前向传播
        function out = neural_networks_forward(input,weights,bias,active_func)
            out_unactive = input*weights+bias;
            out = active_func(out_unactive);
        end
        %反向求导
        function [weights_new,bias_new,last_gradient] = neural_networks_back(learning_rate,input_layer,out,weights,bias,gradient,active_gradient_func)
            %激活函数求导
            gradient_new = active_gradient_func(out).*gradient;
            %更新偏置
            bias_new = bias - learning_rate.*gradient_new;
            %weights 的尺寸 input*output
            [w_h,w_w] = size(weights);
            
            %更新偏置
            x_t = repmat(input_layer',1,w_w);
            gradient_t = repmat(gradient_new,w_h,1);
            %
            last_gradient = gradient_t.*weights;
            last_gradient = sum(last_gradient,2)';
            weights_new = weights - learning_rate*gradient_t.*x_t;
            
        end
        %Leaky relu
        function output = lrelu(x)
            output = x;
            for k=1:1:length(output)
                if output(k) < 0
                    output(k)=0.1*output(k);
                end
            end
        end
        %Leaky relu gradient
        function gradient = lrelu_gradient(output)
            gradient = ones(1,length(output));
            for k=1:1:length(output)
                if output(k)<=0
                    gradient(k) = 0.1;
                end
            end
        end
        %sigmoid
        function output = sigmoid(x)
            output =1./(1+exp(-x));
        end
        %sigmoid gradient
        function gradient = sigmoid_gradient(output)
            gradient =output.*(1-output);
        end
    end
end